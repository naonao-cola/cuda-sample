{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pycuda.compiler import SourceModule\n",
                "import sys\n",
                "from time import time\n",
                "from functools import reduce\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib\n",
                "from matplotlib import pyplot as plt\n",
                "from IPython.core.interactiveshell import InteractiveShell\n",
                "\n",
                "import pycuda\n",
                "import pycuda.autoinit\n",
                "import pycuda.driver as drv\n",
                "from pycuda import gpuarray\n",
                "from pycuda.elementwise import ElementwiseKernel\n",
                "from pycuda.scan import InclusiveScanKernel\n",
                "from pycuda.reduction import ReductionKernel\n",
                "\n",
                "InteractiveShell.ast_node_interactivity = \"all\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# https://github.com/YouQixiaowu/CUDA-Programming-with-Python\n",
                "# https://blog.csdn.net/dzpmfd/article/details/137679110\n",
                "\n",
                "def query_device():\n",
                "    print(f'The version of PyCUDA: {pycuda.VERSION}')\n",
                "    print(f'The version of Python: {sys.version}')\n",
                "    drv.init()\n",
                "    print('CUDA device query (PyCUDA version) \\n')\n",
                "    print(f'Detected {drv.Device.count()} CUDA Capable device(s) \\n')\n",
                "    for i in range(drv.Device.count()):\n",
                "\n",
                "        gpu_device = drv.Device(i)\n",
                "        print(f'Device {i}: {gpu_device.name()}')\n",
                "        compute_capability = float('%d.%d' % gpu_device.compute_capability())\n",
                "        print(f'\\t Compute Capability: {compute_capability}')\n",
                "        print(\n",
                "            f'\\t Total Memory: {gpu_device.total_memory()//(1024**2)} megabytes')\n",
                "\n",
                "        # The following will give us all remaining device attributes as seen\n",
                "        # in the original deviceQuery.\n",
                "        # We set up a dictionary as such so that we can easily index\n",
                "        # the values using a string descriptor.\n",
                "\n",
                "        device_attributes_tuples = gpu_device.get_attributes().items()\n",
                "        device_attributes = {}\n",
                "\n",
                "        for k, v in device_attributes_tuples:\n",
                "            device_attributes[str(k)] = v\n",
                "\n",
                "        num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
                "\n",
                "        # Cores per multiprocessor is not reported by the GPU!\n",
                "        # We must use a lookup table based on compute capability.\n",
                "        # See the following:\n",
                "        # http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
                "\n",
                "        cuda_cores_per_mp = {5.0: 128, 5.1: 128, 5.2: 128,\n",
                "                             6.0: 64, 6.1: 128, 6.2: 128, 8.9: 128}.get(compute_capability, None)\n",
                "\n",
                "        if cuda_cores_per_mp is None:\n",
                "            raise ValueError(\n",
                "                f\"Unsupported compute capability: {compute_capability}\")\n",
                "\n",
                "        print(f'\\t ({num_mp}) Multiprocessors, ({cuda_cores_per_mp}) CUDA Cores / Multiprocessor: {num_mp*cuda_cores_per_mp} CUDA Cores')\n",
                "\n",
                "        device_attributes.pop('MULTIPROCESSOR_COUNT')\n",
                "\n",
                "        for k in device_attributes.keys():\n",
                "            print(f'\\t {k}: {device_attributes[k]}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cuda_1():\n",
                "    \"\"\"\n",
                "    NumPy array 和 gpuarray 之间的相互转换\n",
                "    \"\"\"\n",
                "    host_data = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
                "    device_data = gpuarray.to_gpu(host_data)\n",
                "    device_data_x2 = 2 * device_data\n",
                "    host_data_x2 = device_data_x2.get()\n",
                "    print(host_data_x2)\n",
                "\n",
                "def cuda_2():\n",
                "    \"\"\"\n",
                "    gpuarray 的基本运算\n",
                "    \"\"\"\n",
                "    x_host = np.array([1, 2, 3], dtype=np.float32)\n",
                "    y_host = np.array([1, 1, 1], dtype=np.float32)\n",
                "    z_host = np.array([2, 2, 2], dtype=np.float32)\n",
                "\n",
                "    x_device = gpuarray.to_gpu(x_host)\n",
                "    y_device = gpuarray.to_gpu(y_host)\n",
                "    z_device = gpuarray.to_gpu(z_host)\n",
                "\n",
                "    # x_host + y_host\n",
                "    print((x_device + y_device).get())\n",
                "    # x_host ** z_host\n",
                "    print((x_device ** z_device).get())\n",
                "    # x_host / x_host\n",
                "    print((x_device / x_device).get())\n",
                "    # z_host - x_host\n",
                "    print((z_device - x_device).get())\n",
                "    # z_host / 2\n",
                "    print((z_device / 2).get())\n",
                "    # x_host - 1\n",
                "    print((x_device - 1).get())\n",
                "\n",
                "\n",
                "def simple_speed_test():\n",
                "    host_data = np.float32(np.random.random(50000000))\n",
                "\n",
                "    t1 = time()\n",
                "    host_data_2x = host_data * np.float32(2)\n",
                "    t2 = time()\n",
                "\n",
                "    print(f'total time to compute on CPU: {t2 - t1}')\n",
                "\n",
                "    device_data = gpuarray.to_gpu(host_data)\n",
                "\n",
                "    t1 = time()\n",
                "    device_data_2x = device_data * np.float32(2)\n",
                "    t2 = time()\n",
                "\n",
                "    from_device = device_data_2x.get()\n",
                "\n",
                "    print(f'total time to compute on GPU: {t2 - t1}')\n",
                "    print(\n",
                "        f'Is the host computation the same as the GPU computation? : {np.allclose(from_device, host_data_2x)}')\n",
                "\n",
                "\n",
                "# query_device()\n",
                "# cuda_1()\n",
                "# cuda_2()\n",
                "# simple_speed_test()\n",
                "\n",
                "gpu_2x_ker = ElementwiseKernel(\n",
                "    \"float *in, float *out\",\n",
                "    \"out[i] = 2 * in[i];\",\n",
                "    \"gpu_2x_ker\"\n",
                ")\n",
                "\n",
                "def elementwise_kernel_example():\n",
                "    \"\"\"\n",
                "    ElementwiseKernel：按元素运算.ElementWiseKernel 非常类似于 map 函数\n",
                "    list(map(lambda x: x + 10, [1, 2, 3, 4, 5]))\n",
                "    第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的迭代器\n",
                "\n",
                "    ElementwiseKernel 的参数：\n",
                "\n",
                "    class pycuda.elementwise.ElementwiseKernel(arguments, operation, name=\"kernel\", keep=False, options=[], preamble=\"\")\n",
                "\n",
                "    arguments：该内核定义的传参。\n",
                "    operation：该内核定义的内嵌 CUDA C 代码。\n",
                "    name：定义的内核名称。\n",
                "\n",
                "    gpuarray.empty_like 用于分配与 device_data 相同形状和类型的内存空间。\n",
                "\n",
                "    \"\"\"\n",
                "    host_data = np.float32(np.random.random(50000000))\n",
                "    t1 = time()\n",
                "    host_data_2x = host_data * np.float32(2)\n",
                "    t2 = time()\n",
                "    print(f'total time to compute on CPU: {t2 - t1}')\n",
                "\n",
                "    device_data = gpuarray.to_gpu(host_data)\n",
                "    # allocate memory for output\n",
                "    device_data_2x = gpuarray.empty_like(device_data)\n",
                "\n",
                "    t1 = time()\n",
                "    gpu_2x_ker(device_data, device_data_2x)\n",
                "    t2 = time()\n",
                "    from_device = device_data_2x.get()\n",
                "    print(f'total time to compute on GPU: {t2 - t1}')\n",
                "    print(\n",
                "        f'Is the host computation the same as the GPU computation? : {np.allclose(from_device, host_data_2x)}')\n",
                "\n",
                "\n",
                "# elementwise_kernel_example()\n",
                "# elementwise_kernel_example()\n",
                "# elementwise_kernel_example()\n",
                "# elementwise_kernel_example()\n",
                "# elementwise_kernel_example()\n",
                "\n",
                "def inclusive():\n",
                "    \"\"\"\n",
                "    Python 标准包 functools 中的 reduce 函数。 reduce(lambda x, y : x + y, [1, 2, 3, 4])\n",
                "    与 map 函数不同,reduce 执行迭代的二元运算,只输出一个单值。\n",
                "\n",
                "    InclusiveScanKernel 类似于 reduce,因为它并非输出单值,输出与输入形状相同。\n",
                "\n",
                "    [ 1  3  6 10]\n",
                "    [ 1  3  6 10]\n",
                "    \"\"\"\n",
                "    seq = np.array([1, 2, 3, 4], dtype=np.int32)\n",
                "    seq_gpu = gpuarray.to_gpu(seq)\n",
                "    sum_gpu = InclusiveScanKernel(np.int32, \"a+b\")\n",
                "    print(sum_gpu(seq_gpu).get())\n",
                "    print(np.cumsum(seq))\n",
                "\n",
                "\n",
                "# inclusive()\n",
                "\n",
                "def inclusive_2():\n",
                "    \"\"\"\n",
                "    查找最大值（最大值向后冒泡）：\n",
                "    对于 a > b ? a : b ,我们可以想象是做从前往后做一个遍历（实际是并行的）,而对于每个当前元素 cur,都和前一个元素做比较,把最大值赋值给 cur\n",
                "    这样，最大值就好像“冒泡”一样往后移动，最终取最后一个元素即可。\n",
                "    \"\"\"\n",
                "    seq = np.array([1, 100, -3, -10000, 4, 10000, 66, 14, 21], dtype=np.int32)\n",
                "    seq_gpu = gpuarray.to_gpu(seq)\n",
                "    max_gpu = InclusiveScanKernel(np.int32, \"a > b ? a : b\")\n",
                "    seq_max_bubble = max_gpu(seq_gpu)\n",
                "    print(seq_max_bubble)\n",
                "    print(seq_max_bubble.get()[-1])\n",
                "    print(np.max(seq))\n",
                "\n",
                "\n",
                "# inclusive_2()\n",
                "\n",
                "def reduction():\n",
                "    \"\"\"\n",
                "    实际上,ReductionKernel 就像是执行 ElementWiseKernel 后再执行一个并行扫描内核。一个计算两向量内积的例子：\n",
                "    \"\"\"\n",
                "    a_host = np.array([1, 2, 3], dtype=np.float32)\n",
                "    b_host = np.array([4, 5, 6], dtype=np.float32)\n",
                "    print(a_host.dot(b_host))\n",
                "    dot_prod = ReductionKernel(np.float32, neutral=\"0\", reduce_expr=\"a+b\",\n",
                "                               map_expr=\"x[i]*y[i]\", arguments=\"float *x, float *y\")\n",
                "    a_device = gpuarray.to_gpu(a_host)\n",
                "    b_device = gpuarray.to_gpu(b_host)\n",
                "    print(dot_prod(a_device, b_device).get())\n",
                "\n",
                "\n",
                "# reduction()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "申请内存的写法\n",
                "import numpy as np\n",
                "import pycuda.autoinit\n",
                "import pycuda.gpuarray as gpuarray\n",
                "import pycuda.driver as cuda\n",
                "from pycuda.elementwise import ElementwiseKernel\n",
                "\n",
                "# 定义矩阵\n",
                "A = np.random.randn(3, 3).astype(np.float32)\n",
                "B = np.random.randn(3, 3).astype(np.float32)\n",
                "\n",
                "# 将矩阵上传到 GPU\n",
                "d_A = cuda.mem_alloc(A.nbytes)\n",
                "d_B = cuda.mem_alloc(B.nbytes)\n",
                "cuda.memcpy_htod(d_A, A)\n",
                "cuda.memcpy_htod(d_B, B)\n",
                "\n",
                "# 定义矩阵乘法的内核函数\n",
                "matmul_kernel = ElementwiseKernel(\n",
                "    \"float *A, float *B, float *C\",\n",
                "    \"C[i] = A[i] * B[i]\",\n",
                "    \"matmul_kernel\"\n",
                ")\n",
                "\n",
                "# 执行矩阵乘法\n",
                "C = gpuarray.empty_like(A)\n",
                "matmul_kernel(d_A, d_B, C)\n",
                "\n",
                "# 从 GPU 获取结果\n",
                "result = np.empty_like(C.get())\n",
                "cuda.memcpy_dtoh(result, C)\n",
                "\n",
                "print(result)\n",
                "\"\"\"\n",
                "\n",
                "# 定义 CUDA 核函数\n",
                "mod = SourceModule(\"\"\"\n",
                "    __global__ void add(int *a, int *b, int *c) {\n",
                "        int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
                "        c[idx] = a[idx] + b[idx];\n",
                "    }\n",
                "\"\"\")\n",
                "\n",
                "\n",
                "def test():\n",
                "    # 获取核函数\n",
                "    add_func = mod.get_function(\"add\")\n",
                "\n",
                "    # 定义输入数据\n",
                "    a = np.array([1, 2, 3, 4]).astype(np.int32)\n",
                "    b = np.array([5, 6, 7, 8]).astype(np.int32)\n",
                "    c = np.zeros_like(a)\n",
                "\n",
                "    # 将数据上传到 GPU\n",
                "    d_a = gpuarray.to_gpu(a)\n",
                "    d_b = gpuarray.to_gpu(b)\n",
                "    d_c = gpuarray.to_gpu(c)\n",
                "\n",
                "    # 执行核函数\n",
                "    block_size = 4\n",
                "    # 向下取整\n",
                "    grid_size = len(a) // block_size\n",
                "    add_func(d_a, d_b, d_c, block=(block_size, 1, 1), grid=(grid_size, 1))\n",
                "\n",
                "\n",
                "test()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "qqp-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}