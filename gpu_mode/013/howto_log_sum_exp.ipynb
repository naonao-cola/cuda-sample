{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2a03b8",
   "metadata": {},
   "source": [
    "# How-to Log Sum Exp\n",
    "\n",
    "flash-attention  的softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d499ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 简单的softmax\n",
    "import torch\n",
    "\n",
    "def naive_softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.exp() / x.exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe46e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([0.3142, 0.0223, 0.0794, 0.0450, 0.0121, 0.0764, 0.0184, 0.1597, 0.0350,\n",
      "        0.2375])\n",
      "b tensor([0.3142, 0.0223, 0.0794, 0.0450, 0.0121, 0.0764, 0.0184, 0.1597, 0.0350,\n",
      "        0.2375])\n",
      "allclose True\n"
     ]
    }
   ],
   "source": [
    "## 验证输出\n",
    "\n",
    "x = torch.randn(10)  # generate normally distributed random numbers\n",
    "a = torch.softmax(x, dim=-1) # reference output\n",
    "b = naive_softmax(x) # our naive version\n",
    "\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"allclose\", torch.allclose(a, b, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65d3b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, 0., 0., 0., 0., 0., 0., nan, 0., nan])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当数值较大时，存在输出不稳定\n",
    "naive_softmax(x * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6819d4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have:\n",
      "s1 = tensor([0.0555, 0.1669, 0.2362, 0.5046, 0.0368])\n",
      "s2 = tensor([0.0629, 0.4399, 0.0854, 0.1635, 0.2482])\n",
      "We want:\n",
      "target = tensor([0.0415, 0.1248, 0.1767, 0.3775, 0.0276, 0.0159, 0.1108, 0.0215, 0.0412,\n",
      "        0.0625])\n"
     ]
    }
   ],
   "source": [
    "# 将向量切分为两个小的均等的块进行计算\n",
    "\n",
    "x = torch.randn(10)\n",
    "\n",
    "x1,x2 = torch.chunk(x, 2)\n",
    "s1 = naive_softmax(x1)\n",
    "s2 = naive_softmax(x2)\n",
    "\n",
    "print(\"We have:\")\n",
    "print(f\"s1 = {s1}\")\n",
    "print(f\"s2 = {s2}\")\n",
    "\n",
    "target = naive_softmax(x)\n",
    "print(\"We want:\")\n",
    "print(f\"target = {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7424e692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After correction with help of sum_exp values:\n",
      "s_combined tensor([0.0415, 0.1248, 0.1767, 0.3775, 0.0276, 0.0159, 0.1108, 0.0215, 0.0412,\n",
      "        0.0625])\n",
      "allclose(s_combined, target): True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 合并,可以将 sum exp 称之为 log sum exp\n",
    "\n",
    "sum_exp_x1 = x1.exp().sum()\n",
    "sum_exp_x2 = x2.exp().sum()\n",
    "s1_corrected = s1 * sum_exp_x1 / (sum_exp_x1 + sum_exp_x2)\n",
    "s2_corrected = s2 * sum_exp_x2 / (sum_exp_x1 + sum_exp_x2)\n",
    "\n",
    "print(\"After correction with help of sum_exp values:\")\n",
    "s_combined = torch.cat([s1_corrected, s2_corrected])\n",
    "print(\"s_combined\", s_combined)\n",
    "\n",
    "print(\"allclose(s_combined, target):\", torch.allclose(s_combined, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97617461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([0.3531, 0.0043, 0.0408, 0.0075, 0.0528, 0.1158, 0.0112, 0.0089, 0.0775,\n",
      "        0.0209, 0.0083, 0.0081, 0.0206, 0.0078, 0.0427, 0.0377, 0.0243, 0.0412,\n",
      "        0.1036, 0.0129])\n",
      "b tensor([0.3531, 0.0043, 0.0408, 0.0075, 0.0528, 0.1158, 0.0112, 0.0089, 0.0775,\n",
      "        0.0209, 0.0083, 0.0081, 0.0206, 0.0078, 0.0427, 0.0377, 0.0243, 0.0412,\n",
      "        0.1036, 0.0129])\n",
      "allclose: True\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, Sequence\n",
    "\n",
    "def naive_softmax2(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    sum_exp = x.exp().sum()\n",
    "    return x.exp() / sum_exp, sum_exp\n",
    "\n",
    "\n",
    "def naive_blockwise_softmax(blocks: Sequence[torch.Tensor]) -> torch.Tensor:\n",
    "    total_sum_exp = 0\n",
    "    blocks_out = []\n",
    "    for block in blocks:\n",
    "        block_softmax, block_sum_exp = naive_softmax2(block)\n",
    "        blocks_out.append((block_softmax, block_sum_exp))\n",
    "        total_sum_exp += block_sum_exp\n",
    "\n",
    "    out = []\n",
    "    for block_softmax, block_sum_exp in blocks_out:\n",
    "        out.append(block_softmax * block_sum_exp / total_sum_exp)\n",
    "\n",
    "    return torch.cat(out)\n",
    "\n",
    "x_long = torch.randn(20)\n",
    "chunks = torch.chunk(x_long, 4)\n",
    "a = naive_blockwise_softmax(chunks)\n",
    "b = torch.softmax(x_long, dim=-1)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"allclose:\", torch.allclose(a, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d616e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0115, 0.2500, 0.0473, 0.0237, 0.2880, 0.0985, 0.1659, 0.1152])\n",
      "tensor([0.0115, 0.2500, 0.0473, 0.0237, 0.2880, 0.0985, 0.1659, 0.1152])\n",
      "tensor([0.0115, 0.2500, 0.0473, 0.0237, 0.2880, 0.0985, 0.1659, 0.1152])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8)\n",
    "print(naive_softmax(x))\n",
    "print(naive_softmax(x+5))\n",
    "print(naive_softmax(x-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e329dd",
   "metadata": {},
   "source": [
    "This porperty allows us to deal with problematic large inputs simply by subtracting their maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa106702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    m = x.max()\n",
    "    return (x-m).exp() / (x-m).exp().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db2322",
   "metadata": {},
   "source": [
    "This \"stable\" function now can also deal with larger value that were problematic for our naive function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7468c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive:  tensor([nan, 0., 0., 0., 0., nan, 0., nan, 0., 0.])\n",
      "stable:  tensor([1.0000e+00, 0.0000e+00, 2.8026e-45, 0.0000e+00, 1.1210e-44, 7.7510e-38,\n",
      "        0.0000e+00, 2.6989e-22, 0.0000e+00, 0.0000e+00])\n",
      "torch:  tensor([1.0000e+00, 0.0000e+00, 2.8026e-45, 0.0000e+00, 1.1210e-44, 7.7510e-38,\n",
      "        0.0000e+00, 2.6989e-22, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "large_input = torch.randn(10) * 100\n",
    "\n",
    "print(\"naive: \", naive_softmax(large_input))\n",
    "print(\"stable: \", stable_softmax(large_input))\n",
    "print(\"torch: \", torch.softmax(large_input, dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26cf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax2(x):\n",
    "    \"\"\"returns softmax result and log sum exp\"\"\"\n",
    "    m = x.max()\n",
    "    a = (x - m).exp()\n",
    "    b = a.sum()\n",
    "    lse = m + torch.log(b)\n",
    "    return a / b, lse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2629413",
   "metadata": {},
   "source": [
    "Again we can now use this to combine two softmax block results, but to do it in the same way as before we would need to calculate the exp() values.. which is as we know numerically not stable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3274b21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0140, 0.0597, 0.0266, 0.0128, 0.0549, 0.0315, 0.0457, 0.0235, 0.0570,\n",
      "        0.0340, 0.1228, 0.0127, 0.0575, 0.0276, 0.0136, 0.0049, 0.1780, 0.0686,\n",
      "        0.0300, 0.1246])\n",
      "tensor([0.0140, 0.0597, 0.0266, 0.0128, 0.0549, 0.0315, 0.0457, 0.0235, 0.0570,\n",
      "        0.0340, 0.1228, 0.0127, 0.0575, 0.0276, 0.0136, 0.0049, 0.1780, 0.0686,\n",
      "        0.0300, 0.1246]) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(20)\n",
    "\n",
    "a = torch.softmax(x, dim=-1)\n",
    "\n",
    "x1, x2 = x.chunk(2)\n",
    "\n",
    "b1, lse1 = stable_softmax2(x1)\n",
    "b2, lse2 = stable_softmax2(x2)\n",
    "\n",
    "c1 = b1 * torch.exp(lse1) / (torch.exp(lse1) + torch.exp(lse2))\n",
    "c2 = b2 * torch.exp(lse2) / (torch.exp(lse1) + torch.exp(lse2))\n",
    "\n",
    "print(a)\n",
    "print(torch.cat([c1, c2]), torch.allclose(a, torch.cat([c1, c2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d0c32",
   "metadata": {},
   "source": [
    "But luckily we can rewrite it (`a/(a+b) = 1/(1 + b/a)`) and replace the exp-division by a sbtration of log-values (`exp(a)/exp(b) = exp(a-b)`).\n",
    "\n",
    "\n",
    "这样做的好处是使用对数操作来减少数值溢出，提高稳定性。\n",
    "合并后的结果b与完整计算得到的结果a进行比较，使用torch.allclose()函数验证，结果为True，表示数值稳定的分块合并策略成功达到了与整体计算一致的结果。\n",
    "旁边解释了一个数学技巧：\n",
    "。 提到要在对数尺度上进行减法而非除法，从而保证数值稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb8decab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0140, 0.0597, 0.0266, 0.0128, 0.0549, 0.0315, 0.0457, 0.0235, 0.0570,\n",
      "        0.0340, 0.1228, 0.0127, 0.0575, 0.0276, 0.0136, 0.0049, 0.1780, 0.0686,\n",
      "        0.0300, 0.1246])\n",
      "tensor([0.0140, 0.0597, 0.0266, 0.0128, 0.0549, 0.0315, 0.0457, 0.0235, 0.0570,\n",
      "        0.0340, 0.1228, 0.0127, 0.0575, 0.0276, 0.0136, 0.0049, 0.1780, 0.0686,\n",
      "        0.0300, 0.1246])\n",
      "allclose:  True\n"
     ]
    }
   ],
   "source": [
    "d1 = b1 / (1 + torch.exp(lse2 - lse1))\n",
    "d2 = b2 / (1 + torch.exp(lse1 - lse2))\n",
    "print(a)\n",
    "print(torch.cat([d1, d2]))\n",
    "print(\"allclose: \", torch.allclose(a, torch.cat([d1, d2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0b264",
   "metadata": {},
   "source": [
    "With the fresh knowledge about softmax we can now take a look at the `update()` function that is used in the ring-flash-attention implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c27feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_out_and_lse(\n",
    "    out: torch.Tensor,\n",
    "    lse: torch.Tensor,\n",
    "    block_out: torch.Tensor,\n",
    "    block_lse: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    block_out = block_out.to(torch.float32)\n",
    "    block_lse = block_lse.transpose(-2, -1).unsqueeze(dim=-1)\n",
    "\n",
    "    new_lse = lse + torch.log(1 + torch.exp(block_lse - lse))\n",
    "    out = torch.exp(lse - new_lse) * out + torch.exp(block_lse - new_lse) * block_out\n",
    "\n",
    "    lse = new_lse\n",
    "    return out, lse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
