{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2a03b8",
   "metadata": {},
   "source": [
    "# How-to Log Sum Exp\n",
    "\n",
    "flash-attention  的softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d499ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 简单的softmax\n",
    "import torch\n",
    "\n",
    "def naive_softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.exp() / x.exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe46e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([0.1631, 0.0121, 0.2070, 0.0681, 0.0422, 0.0418, 0.2359, 0.1124, 0.0797,\n",
      "        0.0379])\n",
      "b tensor([0.1631, 0.0121, 0.2070, 0.0681, 0.0422, 0.0418, 0.2359, 0.1124, 0.0797,\n",
      "        0.0379])\n",
      "allclose True\n"
     ]
    }
   ],
   "source": [
    "## 验证输出\n",
    "\n",
    "x = torch.randn(10)  # generate normally distributed random numbers\n",
    "a = torch.softmax(x, dim=-1) # reference output\n",
    "b = naive_softmax(x) # our naive version\n",
    "\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"allclose\", torch.allclose(a, b, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65d3b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, 0., nan, 0., 0., 0., nan, 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当数值较大时，存在输出不稳定\n",
    "naive_softmax(x * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6819d4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have:\n",
      "s1 = tensor([0.0409, 0.4080, 0.2601, 0.1996, 0.0914])\n",
      "s2 = tensor([0.6396, 0.1177, 0.0909, 0.0431, 0.1087])\n",
      "We want:\n",
      "target = tensor([0.0269, 0.2687, 0.1713, 0.1314, 0.0602, 0.2184, 0.0402, 0.0310, 0.0147,\n",
      "        0.0371])\n"
     ]
    }
   ],
   "source": [
    "# 将向量切分为两个小的均等的块进行计算\n",
    "\n",
    "x = torch.randn(10)\n",
    "\n",
    "x1,x2 = torch.chunk(x, 2)\n",
    "s1 = naive_softmax(x1)\n",
    "s2 = naive_softmax(x2)\n",
    "\n",
    "print(\"We have:\")\n",
    "print(f\"s1 = {s1}\")\n",
    "print(f\"s2 = {s2}\")\n",
    "\n",
    "target = naive_softmax(x)\n",
    "print(\"We want:\")\n",
    "print(f\"target = {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7424e692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After correction with help of sum_exp values:\n",
      "s_combined tensor([0.0269, 0.2687, 0.1713, 0.1314, 0.0602, 0.2184, 0.0402, 0.0310, 0.0147,\n",
      "        0.0371])\n",
      "allclose(s_combined, target): True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 合并,可以将 sum exp 称之为 log sum exp\n",
    "\n",
    "sum_exp_x1 = x1.exp().sum()\n",
    "sum_exp_x2 = x2.exp().sum()\n",
    "s1_corrected = s1 * sum_exp_x1 / (sum_exp_x1 + sum_exp_x2)\n",
    "s2_corrected = s2 * sum_exp_x2 / (sum_exp_x1 + sum_exp_x2)\n",
    "\n",
    "print(\"After correction with help of sum_exp values:\")\n",
    "s_combined = torch.cat([s1_corrected, s2_corrected])\n",
    "print(\"s_combined\", s_combined)\n",
    "\n",
    "print(\"allclose(s_combined, target):\", torch.allclose(s_combined, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
