{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention in Torch, Numba and Cuda\n",
    "\n",
    "\n",
    "We implement in three different ways the Flash Attention forward algorithm from the [Flash Attention 2 paper](https://arxiv.org/pdf/2307.08691). Namely using:\n",
    "\n",
    "1. Torch\n",
    "2. Numba\n",
    "3. Cuda\n",
    "\n",
    "- We build the kernel for `d=128` and design it so that each block runs one iteration of the flash attention outer loop.\n",
    "\n",
    "- We then do some basic performance analysis, showing that on small matrices it performs like torch sdpa and disuss register spiling profiling.\n",
    "\n",
    "- Finally we run our custom kernel with [Cuda-python](https://developer.nvidia.com/cuda-python) and [Thunder](https://lightning.ai/docs/thunder/latest/).\n",
    "\n",
    "\n",
    "\n",
    "![./flash_attention_fwd.png](./flash_attention_fwd.png)\n",
    "\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.0233e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from utils import load_cuda, get_sig, print_cuda_info\n",
    "\n",
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (128, 64),\n",
    "    (512, 512),\n",
    "    (1024, 512),\n",
    "]\n",
    "\n",
    "def get_loaded_cuda_module(fname, verbose=False):\n",
    "    cuda_src_path = f\"./{fname}.cu\"\n",
    "    torch_src_path = f\"./torch_extension_template.cu\"\n",
    "    cuda_src = Path(cuda_src_path).read_text()\n",
    "    cuda_src += Path(torch_src_path).read_text()\n",
    "    cuda_src = cuda_src.replace(\"your_function_name\", fname)\n",
    "    cpp_src = get_sig(fname, cuda_src)\n",
    "    return load_cuda(cuda_src, cpp_src, [fname], verbose=verbose)\n",
    "\n",
    "\n",
    "def check_close(O, O_expected, L=None, L_expected=None, atol=5*1e-5):\n",
    "    O_diff = (O - O_expected).abs().max()\n",
    "    print(\"Max absolute difference:\")\n",
    "    if atol:\n",
    "        assert O_diff < atol, f\"O diff too large: {O_diff} > {atol=}\"\n",
    "    print(\"O: \", O_diff)\n",
    "    if L is not None:\n",
    "        L_diff = (L.squeeze() - L_expected).abs().max()\n",
    "        if atol:\n",
    "            assert L_diff < atol, f\"L diff too large: {L_diff} > {atol=}\"\n",
    "        print(\"L: \", L_diff)\n",
    "\n",
    "# Test tensors\n",
    "def get_test_tensors(N_inp, N_out, d):\n",
    "    Q = torch.randn(N_out, d).contiguous().to(\"cuda\")\n",
    "    K = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    V = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    scaling = 1.0 / math.sqrt(d)\n",
    "\n",
    "    # Get expected O\n",
    "    O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "    S = (Q @ K.T) * scaling  # shape: (N_out, N_inp)\n",
    "    L_expected = torch.logsumexp(S, dim=-1)\n",
    "    return Q, K, V, scaling, O_expected, L_expected\n",
    "\n",
    "# Check test tensors\n",
    "N_inp = 512\n",
    "N_out = 512\n",
    "d = 128\n",
    "\n",
    "Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "check_close(O=torch.softmax(Q @ K.T * scaling, dim=-1) @ V, O_expected=O_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.0\n",
      "cuda available: True\n",
      "device count: 1\n",
      "small alloc ok, sum: 64.0\n",
      "Attempting to compile/load module (smoke test). This may fail if driver unstable.)\n",
      "Module loaded (smoke). Running tiny kernel test (8x8).\n",
      "Module smoke call returned.\n",
      "Module loaded (smoke). Running tiny kernel test (8x8).\n",
      "Module smoke call returned.\n"
     ]
    }
   ],
   "source": [
    "import traceback, torch, os\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        a = torch.zeros((8,8), device=\"cuda\")\n",
    "        a += 1.0\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"small alloc ok, sum:\", a.sum().item())\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print(\"Caught exception during small CUDA op\")\n",
    "    # Lightweight smoke-test: try compile/load the CUDA module with small inputs\n",
    "    try:\n",
    "        # Ensure synchronous launches for better error reporting\n",
    "        os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
    "        fname = \"flash_attention\"\n",
    "        print(\"Attempting to compile/load module (smoke test). This may fail if driver unstable.)\")\n",
    "        module_smoke = None\n",
    "        try:\n",
    "            module_smoke = get_loaded_cuda_module(fname, verbose=False)\n",
    "            print(\"Module loaded (smoke). Running tiny kernel test (8x8).\")\n",
    "            Qs, Ks, Vs, scaling, Oexp, Lexp = get_test_tensors(N_inp=8, N_out=8, d=128)\n",
    "            try:\n",
    "                _res = getattr(module_smoke, fname)(Qs, Ks, Vs)\n",
    "                torch.cuda.synchronize()\n",
    "                print(\"Module smoke call returned.\")\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                print(\"Module call raised python exception (if driver crashed, kernel process may die).\")\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            print(\"Module compilation/load failed; skipping smoke kernel run.\")\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print(\"Unexpected error during smoke test setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d) -> None:\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    scaling = 1 / math.sqrt(d)\n",
    "\n",
    "    # Q and O L split into T_r; K, V in T_c blocks\n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r : (i + 1) * B_r]\n",
    "        O_i = torch.zeros(B_r, d)\n",
    "        L_i = torch.zeros(B_r, 1)\n",
    "        m_i = torch.full((B_r, 1), -math.inf)\n",
    "        last_m_i = m_i\n",
    "        for j in range(T_c):\n",
    "            K_j = K[j * B_c : (j + 1) * B_c]\n",
    "            V_j = V[j * B_c : (j + 1) * B_c]\n",
    "            S_i = scaling * (Q_i @ K_j.T)\n",
    "            m_i = torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "            P_i = torch.exp(S_i - m_i)\n",
    "            L_i = torch.exp(last_m_i - m_i) * L_i + P_i.sum(dim=-1, keepdim=True)\n",
    "            O_i = torch.exp(last_m_i - m_i) * O_i + P_i @ V_j\n",
    "            last_m_i = m_i\n",
    "        O_i = (1.0 / L_i) * O_i\n",
    "        L_i = m_i + torch.log(L_i)\n",
    "        O[i * B_r : (i + 1) * B_r] = O_i\n",
    "        L[i * B_r : (i + 1) * B_r] = L_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(3.4273e-07, device='cuda:0')\n",
      "L:  tensor(4.7684e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_torch_loop = torch.zeros(N_out, d)\n",
    "L_torch_loop = torch.zeros(N_out, 1)\n",
    "\n",
    "flash_attention_torch(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"), O_torch_loop, L_torch_loop, N_inp, N_out, d)\n",
    "\n",
    "check_close(\n",
    "    O_torch_loop.to(\"cuda\"),\n",
    "    O_expected,\n",
    "    L_torch_loop.to(\"cuda\"),\n",
    "    L_expected\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba\n",
    "\n",
    "Tiling strategy: threads cooperate loading shared arrays in each `i, j` loop and write their own entries of `l_i, m_i, O_i`\n",
    "\n",
    "### All arrays in shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def flash_attention_numba_all_smem(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    # These can be in registers but wont fit too large\n",
    "    l_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    m_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    O_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "\n",
    "\n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii, dd] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii] = l\n",
    "\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii + i * B_r] = m_i[ii] + math.log(l_i[ii])\n",
    "\n",
    "        numba.cuda.syncthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "\n",
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (128, 64),\n",
    "    (512, 512),\n",
    "    (1024, 512),\n",
    "]\n",
    "\n",
    "block_dim_x = 32\n",
    "block_dim_y = 16\n",
    "\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "    try:\n",
    "        Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "        O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "        L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "        tpb = (block_dim_x, block_dim_y)\n",
    "        grid = (1,)\n",
    "        flash_attention_numba_all_smem[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "        torch.cuda.synchronize()\n",
    "        check_close(\n",
    "            O_all_smem,\n",
    "            O_expected,\n",
    "            L_all_smem,\n",
    "            L_expected,\n",
    "        )\n",
    "    except Exception:\n",
    "        import traceback; traceback.print_exc()\n",
    "        print(\"caught python exception (if any). If kernel caused driver crash, process may have died.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving `m_i`, `l_i`, `O_i` to registers\n",
    "\n",
    "Current shared-memory usage across threads:\n",
    "```\n",
    "Shar = (B_r * d * 4) # Q_i\n",
    "+ (B_c * d * 4) # K_j\n",
    "+ (B_c * d * 4) # V_j\n",
    "+ (B_r * B_c * 4) # S\n",
    "= ~25 KB\n",
    "```\n",
    "\n",
    "Current block-shared accumulators (`m_i`, `l_i`, `O_i`):\n",
    "```\n",
    "Loc = 4 * (B_r + B_r + (B_r * d)) = 8320 B ≈ 8 KB\n",
    "```\n",
    "\n",
    "Total shared usage: **~33 KB** (fine for 1 block/SM).\n",
    "\n",
    "---\n",
    "\n",
    "**Idea:** Move `m_i`, `l_i`, `O_i` to *per-thread* locals to fit in registers.\n",
    "\n",
    "Problem: Full-size per-thread arrays would need\n",
    "\n",
    "```\n",
    "Loc * 32 * 16 ≈ 4 MB > 256 KB register memory per SM\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Optimization:** With tiling `tpb = (32, 16)`:\n",
    "\n",
    "- Each thread handles only  \n",
    "  `d // blockDim.x = 4` columns in `x`  \n",
    "  `B_r // blockDim.y = 1` row in `y`\n",
    "- So per-thread locals can be much smaller:\n",
    "\n",
    "```python\n",
    "l_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "m_i = numba.cuda.local.array((1,), inp_dtype)   # 4 B\n",
    "O_i = numba.cuda.local.array((4,), inp_dtype)   # 16 B\n",
    "```\n",
    "-> Per-thread = 24 B, per block = 24 * 32 * 16 = 12 KB < 256 KB -> avoid register pressure and spills.\n",
    "\n",
    "\n",
    "This is how we set up `flash_attention_numba` below and the cuda version in [`./flash_attention.cu`](./flash_attention.cu)\n",
    "\n",
    "In the performance section we run `./flash_attention_spilling_from_registers.cu` that fits full arrays in local variables, to show the performance decrease by slowing the kernel by ~3×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we also parallelised the loops in `T_r`,\n",
    "# so that each block processes only one outer loop\n",
    "# and we run a grid of T_r size\n",
    "\n",
    "block_dim_x = 64\n",
    "block_dim_y = 8\n",
    "B_r = 8\n",
    "B_c = 64\n",
    "d_over_dim_x = d // block_dim_x\n",
    "B_r_over_dim_y = B_r // block_dim_y\n",
    "\n",
    "@numba.cuda.jit\n",
    "def flash_attention_numba(Q, K, V, scaling: numba.float32, L, O, N_out, N_inp):\n",
    "    B_c = 16\n",
    "    B_r = 8\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    # T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "    dim_y = numba.cuda.blockDim.y\n",
    "    dim_x = numba.cuda.blockDim.x\n",
    "    block_id_x = numba.cuda.blockIdx.x\n",
    "\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "\n",
    "    l_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((B_r_over_dim_y,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((B_r_over_dim_y, d_over_dim_x), inp_dtype)\n",
    "\n",
    "\n",
    "    for i in range(block_id_x, block_id_x+1):\n",
    "        for ii in range(tid_y, B_r, dim_y):\n",
    "            for dd in range(tid_x, d, dim_x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "\n",
    "        for ii in range(B_r_over_dim_y):\n",
    "            for dd in range(d_over_dim_x):\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "\n",
    "        numba.cuda.syncthreads()\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, dim_y):\n",
    "                for dd in range(tid_x, d, dim_x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, dim_x):\n",
    "                for jj in range(tid_y, B_c, dim_y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(B_r_over_dim_y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii * dim_y + tid_y][jj])\n",
    "                m_i[ii] = m\n",
    "                l = numba.float32(math.exp(last_m - m)) * l_i[ii]\n",
    "\n",
    "                for dd in range(d_over_dim_x):\n",
    "                    O_i[ii, dd] *= numba.float32(math.exp(last_m - m))\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = numba.float32(math.exp(S[ii * dim_y + tid_y][jj] - m))\n",
    "                    l += P_ij\n",
    "                    for dd in range(d_over_dim_x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd * dim_x + tid_x]\n",
    "                l_i[ii] = l\n",
    "\n",
    "        for ii in range(B_r_over_dim_y):\n",
    "            for dd in range(d_over_dim_x):\n",
    "                O[ii * dim_y + tid_y + i * B_r, dd * dim_x + tid_x] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii * dim_y + tid_y + i * B_r] = m_i[ii] + numba.float32(math.log(l_i[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, scaling, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "    O_all_smem = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "    L_all_smem = torch.zeros(N_out, device=\"cuda\")\n",
    "    tpb = (block_dim_x, block_dim_y)\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    grid = (T_r,)\n",
    "    flash_attention_numba[grid, tpb](Q, K, V, scaling, L_all_smem, O_all_smem,  N_out, N_inp)\n",
    "    check_close(\n",
    "        O_all_smem,\n",
    "        O_expected,\n",
    "        L_all_smem,\n",
    "        L_expected,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda\n",
    "\n",
    "### flash_attention_numba in Cuda\n",
    "\n",
    "See [flash_attention.cu](./flash_attention.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"flash_attention\"\n",
    "module = None\n",
    "try:\n",
    "    module = get_loaded_cuda_module(fname)\n",
    "    print(\"CUDA module loaded successfully.\")\n",
    "except Exception:\n",
    "    import traceback; traceback.print_exc()\n",
    "    print(\"Failed to load CUDA module; continuing without it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if module is None:\n",
    "    print(\"CUDA module not loaded; skipping CUDA tests.\")\n",
    "else:\n",
    "    for N_inp, N_out in TEST_DIMS:\n",
    "        Q, K, V, _, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "        try:\n",
    "            O_move_registers, L_move_registers = getattr(module, fname)(Q, K, V)\n",
    "            check_close(\n",
    "                O_move_registers,\n",
    "                O_expected,\n",
    "                L_move_registers,\n",
    "                L_expected,\n",
    "            )\n",
    "        except Exception:\n",
    "            import traceback; traceback.print_exc()\n",
    "            print(\"Error running CUDA module for dims:\", N_inp, N_out)\n",
    "            # If driver crashes, the kernel process may die and you'll see 'kernel died' in the notebook logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance \n",
    "\n",
    "Run timeit on different dimensions. \n",
    "\n",
    "- Recall that we build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "- For matrices with small tensors this implementation is comparable with `scaled_dot_product_attention`.\n",
    "\n",
    "- But by making tensors larger our implementation slows down, as it does not use GPU resources efficiently.\n",
    "\n",
    "- Note that the register splilling version is always much slower than `scaled_dot_product_attention`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load registers spilling version\n",
    "\n",
    "This is the version loading full arrays as local variables in threads, which leads to spilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_spill_from_registers = \"flash_attention_spilling_from_registers\"\n",
    "module_spilling_from_registers = None\n",
    "try:\n",
    "    module_spilling_from_registers = get_loaded_cuda_module(fname_spill_from_registers)\n",
    "    print(\"Spilling module loaded.\")\n",
    "except Exception:\n",
    "    import traceback; traceback.print_exc()\n",
    "    print(\"Failed to load spilling-from-registers module; skipping.\")\n",
    "if module_spilling_from_registers is not None:\n",
    "    try:\n",
    "        O_cuda_spilling, L_cuda_spilling = getattr(module_spilling_from_registers, fname_spill_from_registers)(Q, K, V)\n",
    "        check_close(O_cuda_spilling, O_expected, L_cuda_spilling, L_expected)\n",
    "    except Exception:\n",
    "        import traceback; traceback.print_exc()\n",
    "        print(\"Error running spilling-from-registers module.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeit: sdpa backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark first the different backends for scaled_dot_product_attention\n",
    "# Note not much diference between different backends for a small tensor.\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "N_out, N_inp = 32, 32\n",
    "Q, K, V, _, _, _ = get_test_tensors(N_inp=N_inp, N_out=N_out, d=d)\n",
    "Qhalf, Khalf, Vhalf = Q.to(torch.float16), K.to(torch.float16), V.to(torch.float16)\n",
    "\n",
    "def run_sdpa_with_backend(Q, K, V, backend):\n",
    "    with sdpa_kernel(backends=backend):\n",
    "        torch.nn.functional.scaled_dot_product_attention(\n",
    "            Q.unsqueeze(0).unsqueeze(0),\n",
    "            K.unsqueeze(0).unsqueeze(0),\n",
    "            V.unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "    torch.cuda.synchronize()\n",
    "print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "print(\"\\nExplicit softmax(Q @ K.T * scaling) @ V\")\n",
    "%timeit torch.softmax(Q @ K.T * scaling, dim=-1) @ V; torch.cuda.synchronize()\n",
    "print(\"\\nBackend: None\")\n",
    "%timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "print(\"\\nBackend: EFFICIENT_ATTENTION\")\n",
    "%timeit run_sdpa_with_backend(Q, K, V, backend=SDPBackend.EFFICIENT_ATTENTION)\n",
    "print(\"\\nBackend: FLASH_ATTENTION (f16)\")\n",
    "%timeit run_sdpa_with_backend(Qhalf, Khalf, Vhalf, backend=SDPBackend.FLASH_ATTENTION)\n",
    "print(\"\\nBackend: CUDNN_ATTENTION (f16)\")\n",
    "%timeit run_sdpa_with_backend(Qhalf, Khalf, Vhalf, backend=SDPBackend.CUDNN_ATTENTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeit: sdpa vs custom kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check now run against\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _, _, _ = get_test_tensors(N_inp, N_out, d)\n",
    "    print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"\\n- Torch scaled_dot_product_attention\")\n",
    "    %timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention\")\n",
    "    %timeit getattr(module, fname)(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Custom Flash Attention: spill from registers\")\n",
    "    %timeit getattr(module_spilling_from_registers, fname_spill_from_registers)(Q, K, V); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile\n",
    "\n",
    "We can compare the performance of the kernel with and without register spilling as follows:\n",
    "```\n",
    "# Get ptx files or use https://godbolt.org/\n",
    "nvcc -ptx flash_attention.cu -o flash_attention.ptx\n",
    "nvcc -ptx flash_attention_spilling_from_registers.cu -o flash_attention_spilling_from_registers.ptx\n",
    "\n",
    "# Get ncu metrics\n",
    "nvcc -O3 -o test_attention main.cu flash_attention.cu flash_attention_spilling_from_registers.cu\n",
    "ncu ./test_attention\n",
    "```\n",
    "\n",
    "#### PTX comparison\n",
    "\n",
    "For the [spilling kernel](./flash_attention_spilling_from_registers.cu) we see \n",
    "```ptx\n",
    ".local .align 16 .b8 \t__local_depot0[8320];\n",
    "```\n",
    "where 8320 B is exactly the allocation for\n",
    "```cu\n",
    "float l_i[B_r];\n",
    "float m_i[B_r];\n",
    "float O_i[B_r][d];\n",
    "```\n",
    "\n",
    "\n",
    "#### Nsight Compute Comparison\n",
    "\n",
    "| **Metric** | **Spilling Kernel (`flash_attention_spilling_from_registers_k`)** | **Non-Spilling Kernel (`flash_attention_k`)** | **Difference** |\n",
    "|------------|------------------------------------------------------------------|-----------------------------------------------|---------------------------------|\n",
    "| **Duration** | **13.02 ms** | **2.10 ms** | Spilling kernel is ~6× slower |\n",
    "| **Compute (SM) Throughput** | **0.26%** | **1.20%** | 5× higher compute utilization in non-spilling kernel |\n",
    "| **L2 Cache Throughput** | **1.71%** | **0.56%** | Spilling kernel hits L2 more |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda-Python\n",
    "\n",
    "`conda install conda-forge::cuda-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create program\n",
    "from cuda import cuda, nvrtc\n",
    "\n",
    "cuda_src_path = f\"./flash_attention.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "B_r, B_c = 8, 32\n",
    "T_r = (N_out + B_r -1) // B_r\n",
    "T_c = (N_inp + B_r -1) // B_c\n",
    "Q, K, V, scale_factor, O_expected, L_expected = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "err, prog = nvrtc.nvrtcCreateProgram(str.encode(cuda_src), b\"flash_attention.cu\", 0, [], [])\n",
    "\n",
    "# Compile program\n",
    "min, maj = torch.cuda.get_device_capability()\n",
    "opts = [\n",
    "    f\"--gpu-architecture=compute_{min}{maj}\".encode(),\n",
    "    \"--device-as-default-execution-space\".encode(),\n",
    "    \"--std=c++14\".encode()]\n",
    "err, = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "\n",
    "print(err)\n",
    "\n",
    "# Get PTX from compilation\n",
    "err, ptxSize = nvrtc.nvrtcGetPTXSize(prog)\n",
    "ptx = b\" \" * ptxSize\n",
    "err, = nvrtc.nvrtcGetPTX(prog, ptx)\n",
    "print(err)\n",
    "\n",
    "err, logSize = nvrtc.nvrtcGetProgramLogSize(prog)\n",
    "log = b\" \" * logSize\n",
    "err, = nvrtc.nvrtcGetProgramLog(prog, log)\n",
    "print(log.decode())\n",
    "# print(ptx.decode())\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "err, module = cuda.cuModuleLoadData(ptx)\n",
    "print(err)\n",
    "err, kernel = cuda.cuModuleGetFunction(module, b\"flash_attention_k\")\n",
    "print(err, kernel)\n",
    "\n",
    "# Allocate tensors\n",
    "# S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "O_cuda_py = torch.zeros(N_out, d, device=\"cuda\")\n",
    "L_cuda_py = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "# To quote the official tutorial: (https://nvidia.github.io/cuda-python/overview.html)\n",
    "# The following code example is not intuitive\n",
    "# Subject to change in a future release\n",
    "\n",
    "int_args = torch.tensor([0, T_r, T_c], dtype=torch.int32)\n",
    "float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "ptr_args = torch.tensor([i.data_ptr() for i in (O_cuda_py, L_cuda_py, Q, K, V)], dtype=torch.uint64)\n",
    "\n",
    "args = torch.tensor([\n",
    "    *(i.data_ptr() for i in ptr_args),\n",
    "    *(i.data_ptr() for i in float_args),\n",
    "    *(i.data_ptr() for i in int_args)], dtype=torch.uint64)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn():\n",
    "    err = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        T_r,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        128,  # block x dim\n",
    "        8,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "\n",
    "fn()\n",
    "\n",
    "(O_cuda_py - O_expected).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "torch.cuda.synchronize()\n",
    "print(\"\\n- Custom Flash Attention: Cuda-python\")\n",
    "%timeit fn(); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thunder\n",
    "\n",
    "[Installation guide](https://lightning.ai/docs/thunder/latest/fundamentals/installation.html)\n",
    "\n",
    "We use thunder here to include our custom kernel as an option within torch's own `scaled_dot_product_attention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thunder\n",
    "\n",
    "attn_ex = thunder.extend.OperatorExecutor('attn_ex', version=0.01)\n",
    "thunder.add_default_executor(attn_ex)\n",
    "\n",
    "# [attn_ex, attn_ex, sdpa, nvfuser]\n",
    "\n",
    "def my_attn_impl(query, key, value, scale):\n",
    "    n_out, d = query.shape\n",
    "\n",
    "    # S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "    O3 = torch.zeros(N_out, d, device=\"cuda\")\n",
    "    L3 = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    int_args = torch.tensor([N_out, T_r, T_c], dtype=torch.int32)\n",
    "    float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "    ptr_args = torch.tensor([i.data_ptr() for i in (O3, L3, key, query, value)], dtype=torch.uint64)\n",
    "\n",
    "    args = torch.tensor([\n",
    "        *(i.data_ptr() for i in ptr_args),\n",
    "        *(i.data_ptr() for i in float_args),\n",
    "        *(i.data_ptr() for i in int_args)], dtype=torch.uint64\n",
    "    )\n",
    "\n",
    "    err, _ = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        T_r,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        32, # block x dim\n",
    "        32, # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "    assert err == cuda.CUresult.CUDA_SUCCESS, err\n",
    "    return O3, L3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register our implementation as an operator\n",
    "def my_attn_meta(query, key, value, scale):\n",
    "    return thunder.TensorProxy(like=query), thunder.TensorProxy(like=query, shape=(query.shape[:-1],))\n",
    "\n",
    "my_attn = attn_ex.register_operator('my_attn', meta=my_attn_meta, fn=my_attn_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_attn_checker(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if attn_mask is not None or dropout_p == 0.0 or is_causal:\n",
    "        return False\n",
    "    if len(query.shape) > 2:\n",
    "            return (query.device.device_type == thunder.devices.DeviceType.CUDA and\n",
    "                key.device == query.device and\n",
    "                value.device == query.device)\n",
    "    return False\n",
    "\n",
    "def my_attn_transform(query, key, value, attn_masks=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** -0.5\n",
    "    out = my_attn(query, key, value, scale)\n",
    "    return out[0]\n",
    "\n",
    "attn_ex.register_implementation(thunder.torch.scaled_dot_product_attention, checker=my_attn_checker,\n",
    "                                  execution_transform=my_attn_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(query, key, value):\n",
    "        return torch.nn.functional.scaled_dot_product_attention(query, key, value, is_causal=False)\n",
    "\n",
    "jfn = thunder.jit(test_fn)\n",
    "\n",
    "print((jfn(Q, K, V).to(\"cpu\") - test_fn(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"))).abs().max())\n",
    "print(thunder.last_traces(jfn)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cuda_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
