```c++

#include "../common/common.h"

/**
使用CUDA事件来控制GPU上启动的异步工作的示例。在本例中，异步副本和异步内核是使用。CUDA事件用于确定工作何时完成。

CPU executed 52135 iterations while waiting for GPU to finish
*/
void asyncAPI();

/**
使用CUDA回调来触发主机上的工作的示例完成设备上的异步工作。在这个例子中，n_streamsCUDA流被创建并在每个流中异步启动4个内核。
然后，在这些异步内核完成时添加回调打印诊断信息。事件计时器，并且事件同步

> Using Device 0: NVIDIA GeForce RTX 4060 Laptop GPU
> Compute Capability 8.9 hardware with 24 multi-processors
> CUDA_DEVICE_MAX_CONNECTIONS = 8
> with streams = 4
callback from stream 0
callback from stream 1
callback from stream 3
callback from stream 2
Measured time for parallel execution = 2.796s

*/
void simpleCallback();

/**

这个例子演示了将工作提交到CUDA流的广度优先秩序。按广度优先顺序提交工作可以防止错误依赖减少应用程序的并行性。kernel_1 kernel_2,
Kernel_3和kernel_4简单地实现相同的虚拟计算。使用单独的内核可以使这些内核的调度更简单在可视化分析器中可视化。

xmake run 06 4  2

CUDA_DEVICE_MAX_CONNECTIONS = 32
> Using Device 0: NVIDIA GeForce RTX 4060 Laptop GPU with num_streams 4
> Compute Capability 8.9 hardware with 24 multi-processors
> grid 8 block 512
Measured time for parallel execution = 14.485s

*/
void simpleHyperqBreadth(int argv1 = 4, int argv2 = 2);

/**
流间添加依赖
添加流间依赖项的简单示例cudaStreamWaitEvent。这段代码在每个n_streams中启动4个内核流。在每个流完成时记录一个事件(kernelEvent)。
然后在该事件和最后一个流上调用cudaStreamWaitEvent(streams[n_streams - 1])强制最终流中的所有计算只进行当所有其他流完成时执行。

xmake run 06 4  1

CUDA_DEVICE_MAX_CONNECTIONS = 32
> Using Device 0: NVIDIA GeForce RTX 4060 Laptop GPU with num_streams 4
> Compute Capability 8.9 hardware with 24 multi-processors
> grid 8 block 512
Measured time for parallel execution = 20.043s

*/
void simpleHyperqDependence(int argv1 = 4, int argv2 = 2);

/**

这个例子演示了提交工作到CUDA流的深度优先秩序。按深度优先顺序提交工作可能会引入错误依赖在不同CUDA流中不相关的任务之间，限制了并行性
一个CUDA应用程序。Kernel_1, kernel_2, kernel_3，和kernel_4实现相同的虚拟计算。使用单独的内核可以使这些内核的调度更容易在Visual Profiler中可视化。
xmake run 06 4  1

CUDA_DEVICE_MAX_CONNECTIONS = 32
> Using Device 0: NVIDIA GeForce RTX 4060 Laptop GPU with num_streams=4
> Compute Capability 8.9 hardware with 24 multi-processors
> grid 8 block 512
Measured time for parallel execution = 14.678s
*/
void simpleHyperqDepth(int argv1 = 4, int argv2 = 2);


/**
一个使用OpenMP并行创建CUDA工作的例子多个流。本例使用n_streams的OpenMP线程启动4每个流中的内核。注意引入的新pragma， #pragma omp parallel。

CUDA_DEVICE_MAX_CONNECTIONS = 32
> Using Device 0: NVIDIA GeForce RTX 4060 Laptop GPU with num_streams=4
> Compute Capability 8.9 hardware with 24 multi-processors
> grid 8 block 512
Measured time for parallel execution = 7.677s

*/
void simpleHyperqOpenmp();


/**
将数据拷贝 数据计算 结果拷贝 分到4个流中，每个流计算一部分

这个例子演示了重叠计算和通信对数据集进行分区并异步启动内存副本和每个子集的核。启动给定的所有传输和内核在同一CUDA流中的子集确保在必要的数据被传输之前不会启动设
备上的计算。但是，由于每个子集的工作独立于所有其他子集，因此不同子集的通信和计算将重叠。本例以广度优先顺序启动副本和内核。

> Using Device 0: NVIDIA GeForce RTX 4060 Laptop GPU
> Compute Capability 8.9 hardware with 24 multi-processors
> CUDA_DEVICE_MAX_CONNECTIONS = 1
> with streams = 4
> vector size = 262144
> grid (2048, 1) block (128, 1)

Measured timings (throughput):
 Memcpy host to device  : 3.784768 ms (0.277052 GB/s)
 Memcpy device to host  : 0.246976 ms (4.245659 GB/s)
 Kernel                 : 1197.521606 ms (0.001751 GB/s)
 Total                  : 1201.553345 ms (0.001745 GB/s)

Actual results from overlapped data transfers:
 overlap with 4 streams : 290.024536 ms (0.007231 GB/s)
 speedup                : 75.862534
Arrays match.

*/
void simpleMultiAddBreadth();


/**
广度优先要快一些
这个例子演示了重叠计算和通信对数据集进行分区并异步启动内存副本和每个子集的核。启动给定的所有传输和内核在同一CUDA流中的子集确保在必要的数
据被传输之前不会启动设备上的计算。但是，由于每个子集的工作独立于所有其他子集，因此不同子集的通信和计算将重叠。
这个示例按深度优先顺序启动副本和内核。

> Using Device 0: NVIDIA GeForce RTX 4060 Laptop GPU
> Compute Capability 8.9 hardware with 24 multi-processors
> CUDA_DEVICE_MAX_CONNECTIONS = 1
> with streams = 4
> vector size = 262144
> grid (2048, 1) block (128, 1)

Measured timings (throughput):
 Memcpy host to device  : 1.493152 ms (0.702257 GB/s)
 Memcpy device to host  : 0.413440 ms (2.536223 GB/s)
 Kernel                 : 1231.366211 ms (0.001703 GB/s)
 Total                  : 1233.272827 ms (0.001700 GB/s)

Actual results from overlapped data transfers:
 overlap with 4 streams : 329.370941 ms (0.006367 GB/s)
 speedup                : 73.292938
Arrays match.

*/
void simpleMultiAddDepth();

```